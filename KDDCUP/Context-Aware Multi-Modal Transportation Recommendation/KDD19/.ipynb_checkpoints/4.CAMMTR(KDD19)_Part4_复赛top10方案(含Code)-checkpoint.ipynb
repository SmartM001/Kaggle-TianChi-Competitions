{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#工具包导入&amp;数据读取\" data-toc-modified-id=\"工具包导入&amp;数据读取-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>工具包导入&amp;数据读取</a></span><ul class=\"toc-item\"><li><span><a href=\"#工具包导入\" data-toc-modified-id=\"工具包导入-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>工具包导入</a></span></li><li><span><a href=\"#数据读取\" data-toc-modified-id=\"数据读取-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>数据读取</a></span></li></ul></li><li><span><a href=\"#数据集预处理\" data-toc-modified-id=\"数据集预处理-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>数据集预处理</a></span><ul class=\"toc-item\"><li><span><a href=\"#构建plans_pivot表格\" data-toc-modified-id=\"构建plans_pivot表格-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>构建plans_pivot表格</a></span></li></ul></li><li><span><a href=\"#特征工程\" data-toc-modified-id=\"特征工程-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>特征工程</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据拼接\" data-toc-modified-id=\"数据拼接-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>数据拼接</a></span><ul class=\"toc-item\"><li><span><a href=\"#提前划分验证集,防止过拟合\" data-toc-modified-id=\"提前划分验证集,防止过拟合-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>提前划分验证集,防止过拟合</a></span></li></ul></li><li><span><a href=\"#构建基于sid的特征\" data-toc-modified-id=\"构建基于sid的特征-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>构建基于sid的特征</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于plans_pivot表格\" data-toc-modified-id=\"基于plans_pivot表格-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>基于plans_pivot表格</a></span></li><li><span><a href=\"#基于plans,queries表格\" data-toc-modified-id=\"基于plans,queries表格-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>基于plans,queries表格</a></span></li></ul></li><li><span><a href=\"#基于pid的特征\" data-toc-modified-id=\"基于pid的特征-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>基于pid的特征</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于plans,queries表格\" data-toc-modified-id=\"基于plans,queries表格-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>基于plans,queries表格</a></span></li></ul></li><li><span><a href=\"#基于od的特征\" data-toc-modified-id=\"基于od的特征-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>基于od的特征</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于plans,queries表格\" data-toc-modified-id=\"基于plans,queries表格-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>基于plans,queries表格</a></span></li><li><span><a href=\"#word2vec特征（捕捉o&amp;d的联系）\" data-toc-modified-id=\"word2vec特征（捕捉o&amp;d的联系）-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>word2vec特征（捕捉o&amp;d的联系）</a></span></li></ul></li><li><span><a href=\"#其他组合特征\" data-toc-modified-id=\"其他组合特征-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>其他组合特征</a></span></li><li><span><a href=\"#加入profile文件\" data-toc-modified-id=\"加入profile文件-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>加入profile文件</a></span></li><li><span><a href=\"#转cate\" data-toc-modified-id=\"转cate-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>转cate</a></span></li></ul></li><li><span><a href=\"#线下&amp;线上验证\" data-toc-modified-id=\"线下&amp;线上验证-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>线下&amp;线上验证</a></span><ul class=\"toc-item\"><li><span><a href=\"#加入权重,线下验证:0.7上下波动\" data-toc-modified-id=\"加入权重,线下验证:0.7上下波动-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>加入权重,线下验证:0.7上下波动</a></span></li><li><span><a href=\"#线上提交:0.7018左右波动\" data-toc-modified-id=\"线上提交:0.7018左右波动-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>线上提交:0.7018左右波动</a></span></li></ul></li><li><span><a href=\"#小结\" data-toc-modified-id=\"小结-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>小结</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具包导入&数据读取\n",
    "## 工具包导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import gc\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    " \n",
    "from collections import Counter \n",
    "from statistics import mode \n",
    "    \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "import ast \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/Data_JieZhang/KDD/'\n",
    "\n",
    "train_queries = pd.read_csv(path + 'train_queries.csv', parse_dates=['req_time'])\n",
    "train_plans   = pd.read_csv(path + 'train_plans.csv', parse_dates=['plan_time'])\n",
    "train_clicks  = pd.read_csv(path + 'train_clicks.csv')\n",
    "profiles      = pd.read_csv(path + 'profiles.csv') \n",
    "\n",
    "test_queries  = pd.read_csv(path + 'test_queries.csv', parse_dates=['req_time'])\n",
    "test_plans    = pd.read_csv(path + 'test_plans.csv', parse_dates=['plan_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集预处理\n",
    "主要针对tran_plans数据集进行转换,方便提取特征."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plans['plans'] = train_plans['plans'].fillna('[]').apply(ast.literal_eval)\n",
    "test_plans['plans']  = test_plans['plans'].fillna('[]').apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans = pd.concat([train_plans, test_plans],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建plans_pivot表格\n",
    "- 这么构建,方便快速构建很多特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plans_trans(df):\n",
    "    df_plans_pivot   = pd.DataFrame()\n",
    "    sids             = []\n",
    "    prices           = []\n",
    "    distances        = []\n",
    "    etas             = []\n",
    "    transport_modes  = []\n",
    "    ranks            = []   \n",
    "    plan_times       = []   \n",
    "    for sid,plan_time,plans in tqdm(df[['sid','plan_time','plans']].values):\n",
    "         \n",
    "        for i,plan in enumerate(plans):\n",
    "            sids.append(sid)\n",
    "            transport_modes.append(plan['transport_mode'])\n",
    "            distances.append(plan['distance'])\n",
    "            etas.append(plan['eta'])\n",
    "            prices.append(plan['price']) \n",
    "            plan_times.append(plan_time)\n",
    "            ranks.append(i)\n",
    "    df_plans_pivot['sid']             = sids\n",
    "    df_plans_pivot['price']           = prices\n",
    "    df_plans_pivot['distance']        = distances\n",
    "    df_plans_pivot['transport_mode']  = transport_modes\n",
    "    df_plans_pivot['ranks']           = ranks\n",
    "    df_plans_pivot['plan_time']       = plan_times \n",
    "    df_plans_pivot['etas']            = etas \n",
    "    df_plans_pivot['price']           = df_plans_pivot['price'].apply(lambda x: 0 if x == '' else float(x))\n",
    "    return df_plans_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们对速度较快,行程较大并且价格为0的数据的价格进行修正,速度超快,而价格为0是不合理的,这边预处理在线下和线上都可以得到提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583625/583625 [00:02<00:00, 247861.14it/s]\n"
     ]
    }
   ],
   "source": [
    "plans_pivot = _plans_trans(plans)\n",
    "plans_pivot['speed'] = plans_pivot['distance'] / plans_pivot['etas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_pivot['max_speed'] = plans_pivot.groupby('sid')['speed'].transform(max)\n",
    "plans_pivot['max_price'] = plans_pivot.groupby('sid')['price'].transform(max)\n",
    "plans_pivot['new_price'] = plans_pivot['price'].values\n",
    "ind = (plans_pivot['max_speed']   == plans_pivot['speed']) & (plans_pivot['price'] == 0) & (plans_pivot['max_speed'] >= 2.2)\n",
    "plans_pivot.loc[ind,'new_price' ] =  plans_pivot.loc[ind,'max_price'].values + 1000\n",
    "plans_pivot['price'] = plans_pivot['new_price'].values\n",
    "del plans_pivot['max_speed']\n",
    "del plans_pivot['max_price']\n",
    "del plans_pivot['new_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程 \n",
    "## 数据拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_queries.merge(train_plans, 'left', ['sid'])\n",
    "test  = test_queries.merge(test_plans, 'left', ['sid']) \n",
    "train = train.merge(train_clicks, 'left', ['sid'])\n",
    "train['click_mode'] = train['click_mode'].fillna(0).astype(int)\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "data['pid'] = data['pid'].fillna(-1).astype(int) # 此处可能不合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提前划分验证集,防止过拟合\n",
    "- 注意此处我们用req_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_val = (data.req_time >= '2018-11-23') & (data.req_time < '2018-12-01')\n",
    "validation_data = data.loc[ind_val, ['sid','click_mode']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_val_test = (data.req_time >= '2018-11-23')\n",
    "data.loc[ind_val_test, 'click_mode'] = np.nan\n",
    "data.loc[ind_val_test, 'click_time'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main = data[['sid','pid','req_time','o','d','click_mode','click_time']].copy()\n",
    "data_main['od'] = data_main['o'] +'_' + data_main['d']\n",
    "main_features = ['pid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建基于sid的特征\n",
    "### 基于plans_pivot表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_second_common(x,i):\n",
    "    t = Counter(x)\n",
    "    try:\n",
    "        return t.most_common()[i][0]\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def _get_sid_features(df):\n",
    "    df_sid                      =  pd.DataFrame()\n",
    "    df_sid['sid']               =  df['sid'].unique()\n",
    "    \n",
    "    # sid: cnt\n",
    "    sid_cnt_dic                 = df.groupby('sid')['transport_mode'].count().to_dict()\n",
    "    df_sid['sid_transport_cnt'] = df_sid['sid'].map(sid_cnt_dic).values\n",
    "    \n",
    "    #sid(ranks):transport\n",
    "    len_ = df['ranks'].max()\n",
    "    for i in tqdm(range(len_ + 1)):\n",
    "        df_          =  df.loc[df.ranks == i].copy()\n",
    "        sid_rank_transmode_dic =  dict(zip(df_['sid'],df_['transport_mode']))  \n",
    "        sid_rank_price_dic     =  dict(zip(df_['sid'],df_['price']))            \n",
    "        sid_rank_eta_dic       =  dict(zip(df_['sid'],df_['etas']))             \n",
    "        sid_rank_distance_dic  =  dict(zip(df_['sid'],df_['distance']))         \n",
    "        \n",
    "        df_sid['sid_rank{}_transport_mode'.format(i)] = df_sid['sid'].map(sid_rank_transmode_dic).values\n",
    "        df_sid['sid_rank{}_price'.format(i)]          = df_sid['sid'].map(sid_rank_price_dic).values\n",
    "        df_sid['sid_rank{}_eta'.format(i)]            = df_sid['sid'].map(sid_rank_eta_dic).values\n",
    "        df_sid['sid_rank{}_distance'.format(i)]       = df_sid['sid'].map(sid_rank_distance_dic).values\n",
    "     \n",
    "    for i in tqdm(range(1,len_)): \n",
    "        df_1          =  df.loc[df.ranks == i].copy()\n",
    "        df_2          =  df.loc[df.ranks == i - 1].copy() \n",
    "        sid_rank_price_dic1     =  dict(zip(df_1['sid'],df_1['price']))             \n",
    "        sid_rank_eta_dic1       =  dict(zip(df_1['sid'],df_1['etas']))              \n",
    "        sid_rank_distance_dic1  =  dict(zip(df_1['sid'],df_1['distance']))      \n",
    "\n",
    "        sid_rank_price_dic2     =  dict(zip(df_2['sid'],df_2['price']))             \n",
    "        sid_rank_eta_dic2       =  dict(zip(df_2['sid'],df_2['etas']))              \n",
    "        sid_rank_distance_dic2  =  dict(zip(df_2['sid'],df_2['distance']))         \n",
    "\n",
    "        df_sid['sid_rank{}_{}_price'.format(i-1,i)]          = df_sid['sid'].map(sid_rank_price_dic1).values - df_sid['sid'].map(sid_rank_price_dic2).values\n",
    "        df_sid['sid_rank{}_{}_eta'.format(i-1,i)]            = df_sid['sid'].map(sid_rank_eta_dic1).values   - df_sid['sid'].map(sid_rank_eta_dic2).values\n",
    "        df_sid['sid_rank{}_{}_distance'.format(i-1,i)]       = df_sid['sid'].map(sid_rank_distance_dic1).values - df_sid['sid'].map(sid_rank_distance_dic2).values\n",
    "\n",
    "    df['price_div_distance']  =    df['price'].values / df['distance'].values\n",
    "    df['etas_div_distance']   =    df['etas'].values  / df['distance'].values\n",
    "    df['etas_div_price']      =    df['etas'].values  / df['price'].values \n",
    "    \n",
    "    sid_price_div_distance_min_dic        = df.groupby('sid')['price_div_distance'].min().to_dict() \n",
    "    df_sid['sid_price_div_distance_min']  = df_sid['sid'].map(sid_price_div_distance_min_dic).values \n",
    "     \n",
    "    sid_price_div_distance_std_dic        = df.groupby('sid')['price_div_distance'].std().to_dict() \n",
    "    df_sid['sid_price_div_distance_std']  = df_sid['sid'].map(sid_price_div_distance_std_dic).values \n",
    "    \n",
    "    sid_etas_div_distance_min_dic         = df.groupby('sid')['etas_div_distance'].min().to_dict() \n",
    "    df_sid['sid_etas_div_distance_min']   = df_sid['sid'].map(sid_etas_div_distance_min_dic).values\n",
    "    \n",
    "    sid_etas_div_distance_std_dic         = df.groupby('sid')['etas_div_distance'].std().to_dict() \n",
    "    df_sid['sid_etas_div_distance_std']   = df_sid['sid'].map(sid_etas_div_distance_std_dic).values\n",
    "    \n",
    "    sid_etas_div_price_min_dic            = df.groupby('sid')['etas_div_price'].min().to_dict() \n",
    "    df_sid['sid_etas_div_price_min']      = df_sid['sid'].map(sid_etas_div_price_min_dic).values\n",
    "    sid_etas_div_price_std_dic            = df.groupby('sid')['etas_div_price'].std().to_dict() \n",
    "    df_sid['sid_etas_div_price_std']      = df_sid['sid'].map(sid_etas_div_price_std_dic).values\n",
    "     \n",
    "    \n",
    "    ##################################新加的##########################################     \n",
    "    sid_etas_div_distance_max_dic         = df.groupby('sid')['etas_div_distance'].max().to_dict() \n",
    "    df_sid['sid_etas_div_distance_max']   = df_sid['sid'].map(sid_etas_div_distance_max_dic).values\n",
    "    sid_etas_div_price_max_dic            = df.groupby('sid')['etas_div_price'].max().to_dict() \n",
    "    df_sid['sid_etas_div_price_max']      = df_sid['sid'].map(sid_etas_div_price_max_dic).values\n",
    "    sid_price_div_distance_max_dic        = df.groupby('sid')['price_div_distance'].max().to_dict() \n",
    "    df_sid['sid_price_div_distance_max']  = df_sid['sid'].map(sid_price_div_distance_max_dic).values \n",
    "        \n",
    "    df_sid['sid_price_div_distance_max_min']  = df_sid['sid_price_div_distance_max'] - df_sid['sid_price_div_distance_min'] \n",
    "    df_sid['sid_etas_div_price_max_min']      = df_sid['sid_etas_div_price_max'] - df_sid['sid_etas_div_price_min'] \n",
    "    df_sid['sid_etas_div_distance_max_min']   = df_sid['sid_etas_div_distance_max'] - df_sid['sid_etas_div_distance_min']     \n",
    "    del df_sid['sid_etas_div_price_max']\n",
    "    del df_sid['sid_price_div_distance_max']\n",
    "    del df_sid['sid_etas_div_distance_max']\n",
    "    del df_sid['sid_etas_div_price_min']\n",
    "    del df_sid['sid_price_div_distance_min']\n",
    "    del df_sid['sid_etas_div_distance_min']\n",
    "    \n",
    "    \n",
    "    sid_distance_min_dic            = df.groupby('sid')['distance'].min().to_dict() \n",
    "    df_sid['sid_distance_min']      = df_sid['sid'].map(sid_distance_min_dic).values \n",
    "    sid_price_min_dic               = df.groupby('sid')['price'].min().to_dict() \n",
    "    df_sid['sid_price_min']         = df_sid['sid'].map(sid_price_min_dic).values \n",
    "    sid_etas_min_dic                = df.groupby('sid')['etas'].min().to_dict() \n",
    "    df_sid['sid_etas_min']          = df_sid['sid'].map(sid_etas_min_dic).values\n",
    "    \n",
    "    sid_distance_max_dic            = df.groupby('sid')['distance'].max().to_dict() \n",
    "    df_sid['sid_distance_max']      = df_sid['sid'].map(sid_distance_max_dic).values \n",
    "    sid_price_max_dic               = df.groupby('sid')['price'].max().to_dict() \n",
    "    df_sid['sid_price_max']         = df_sid['sid'].map(sid_price_max_dic).values \n",
    "    sid_etas_max_dic                = df.groupby('sid')['etas'].max().to_dict() \n",
    "    df_sid['sid_etas_max']          = df_sid['sid'].map(sid_etas_max_dic).values\n",
    "    \n",
    "    df_sid['sid_distance_max_min']  = df_sid['sid_distance_max'] - df_sid['sid_distance_min'] \n",
    "    df_sid['sid_price_max_min']     = df_sid['sid_price_max'] - df_sid['sid_price_min'] \n",
    "    df_sid['sid_etas_max_min']      = df_sid['sid_etas_max'] - df_sid['sid_etas_min']  \n",
    "    \n",
    "    del df_sid['sid_distance_max']\n",
    "    del df_sid['sid_price_max']\n",
    "    del df_sid['sid_etas_max']\n",
    "    del df_sid['sid_distance_min']\n",
    "    del df_sid['sid_price_min']\n",
    "    del df_sid['sid_etas_min']\n",
    "    ##################################新加的##########################################      \n",
    "     \n",
    "    sid_etas_mean_dic                = df.groupby('sid')['etas'].mean().to_dict() \n",
    "    df_sid['sid_etas_mean']          = df_sid['sid'].map(sid_etas_mean_dic).values \n",
    "    sid_price_mean_dic                = df.groupby('sid')['price'].mean().to_dict() \n",
    "    df_sid['sid_price_mean']          = df_sid['sid'].map(sid_price_mean_dic).values \n",
    "    sid_distance_mean_dic                = df.groupby('sid')['distance'].mean().to_dict() \n",
    "    df_sid['sid_distance_mean']          = df_sid['sid'].map(sid_distance_mean_dic).values\n",
    "    \n",
    "    for fea in tqdm(['distance','price','etas']): #,'price_div_distance','etas_div_distance','etas_div_price']):\n",
    "        df[fea + '_min']                          =  df.groupby('sid')[fea].transform(min)\n",
    "        df_          =  df.loc[df[fea + '_min']   == df[fea]].drop_duplicates(subset=['sid',fea],keep='first').copy()\n",
    "        sid_transmode_dic                         =  dict(zip(df_['sid'],df_['transport_mode'])) \n",
    "        df_sid['sid_min_'+fea +'_transport_mode'] = df_sid['sid'].map(sid_transmode_dic).values\n",
    "    \n",
    "    features = ['sid_min_distance_transport_mode','sid_min_price_transport_mode','sid_min_etas_transport_mode'] #,'sid_min_price_div_distance_transport_mode','sid_min_etas_div_price_transport_mode','sid_min_price_div_distance_transport_mode']\n",
    "    df_sid['most_likely_mode']        = df_sid[features].progress_apply(lambda x: get_first_second_common(x,0),axis=1)\n",
    "    df_sid['second_most_likely_mode'] = df_sid[features].progress_apply(lambda x: get_first_second_common(x,1),axis=1) \n",
    "    return df_sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:10<00:00,  1.47s/it]\n",
      "100%|██████████| 5/5 [00:13<00:00,  2.67s/it]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.05it/s]\n",
      "100%|██████████| 583625/583625 [00:14<00:00, 39023.49it/s]\n",
      "100%|██████████| 583625/583625 [00:14<00:00, 41152.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 3.02 s, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df_sid_plans_pivot1       = _get_sid_features(plans_pivot)\n",
    "sid_plans_pivot_features1 = [col for col in df_sid_plans_pivot1.columns if col not in ['sid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于plans,queries表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['plan_time']                       =  pd.to_datetime(data['plan_time'])\n",
    "data['req_time']                        =  pd.to_datetime(data['req_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sid_queries_plans(df):\n",
    "    df_sid                                =  pd.DataFrame()\n",
    "    df_sid['sid']                         =  df['sid'].values  \n",
    "    df_sid['o']                           =  df['o'].values  \n",
    "    df_sid['d']                           =  df['d'].values  \n",
    "    \n",
    "    # ①基于plan_time & req_time的特征\n",
    "    # 1. 基础属性特征,对时间进行拆分,或者时间之间的加减乘除操作\n",
    "    # 1.1 plan_time - req_time:时间差,可能反应手机的性能\n",
    "    # 1.2.dow: 反应星期几\n",
    "    # 1.3.hour_minutes:反应当日时间信息,一般比hour和minutes好\n",
    "    # 1.4.day\n",
    "    \n",
    "    # 1.1 plan_time - req_time:时间差,可能反应手机的性能\n",
    "    df_sid['plan_time_minus_req_time']    =  df['plan_time'].astype(int) - df['req_time'].astype(int)\n",
    "    df_sid['plan_time']                   =  df['plan_time'].values\n",
    "    # 1.2.dow: 反应星期几\n",
    "    df_sid['dow']                         =  df['plan_time'].dt.weekday \n",
    "    \n",
    "    # 1.3.hour_minutes:反应当日时间信息,一般比hour和minutes好\n",
    "    df_sid['hour']                        =  df['plan_time'].dt.hour\n",
    "    df_sid['minutes']                     =  df['plan_time'].dt.minute\n",
    "    df_sid['hour_minutes']                =  df_sid['hour'] * 60 + df_sid['minutes']\n",
    "      \n",
    "    \n",
    "    # ②基于o,d的特征\n",
    "    # 1. 基础属性特征,对经纬度进行操作,提取相关的信息\n",
    "    # 1.1 o/d_lng,o/d_lat, 基础的经纬度信息, \n",
    "    df_sid['o_lng']                       =  df['o'].map(lambda x: float(x.split(',')[0]))\n",
    "    df_sid['o_lat']                       =  df['o'].map(lambda x: float(x.split(',')[1])) \n",
    "    df_sid['d_lng']                       =  df['d'].map(lambda x: float(x.split(',')[0]))\n",
    "    df_sid['d_lat']                       =  df['d'].map(lambda x: float(x.split(',')[1]))\n",
    "    \n",
    "    df_sid['lng_diff']                    = df_sid['d_lng'] - df_sid['o_lng']\n",
    "    df_sid['lat_diff']                    = df_sid['d_lat'] - df_sid['o_lat']\n",
    "     \n",
    "    od_unique = []\n",
    "    ounique = df_sid['o'].unique()\n",
    "    dunique = df_sid['d'].unique()\n",
    "    for v in ounique:\n",
    "        od_unique.append(v)\n",
    "    for v in dunique:\n",
    "        od_unique.append(v)\n",
    "    \n",
    "    od_unique = list(set(od_unique))\n",
    "    \n",
    "    lbod = LabelEncoder().fit(od_unique)\n",
    "    df_sid['o_lbe']                       =  lbod.transform(df_sid['o'].values) \n",
    "    df_sid['d_lbe']                       =  lbod.transform(df_sid['d'].values)  \n",
    "    \n",
    "    df_sid['od']                          =  df_sid['o'].fillna('-') + df_sid['d'].fillna('-')\n",
    "    lbod = LabelEncoder().fit(df_sid['od'].values)\n",
    "    df_sid['od_lbe']                      =  lbod.transform(df_sid['od'].values)\n",
    " \n",
    "    del df_sid['minutes']\n",
    "    del df_sid['plan_time']\n",
    "    del df_sid['od'] \n",
    "    del df_sid['o'] \n",
    "    del df_sid['d'] \n",
    "    return df_sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 48 ms, total: 6.4 s\n",
      "Wall time: 6.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df_sid_queries_plans       = _get_sid_queries_plans(data)\n",
    "sid_queries_plans_features = [col for col in df_sid_queries_plans.columns if col not in ['sid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于pid的特征\n",
    "### 基于plans,queries表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pid_queries_plans(df):\n",
    "    df_fea                           =  pd.DataFrame() \n",
    "    df_fea['pid']                    =  df['pid'].unique()\n",
    "    # pid: cnt\n",
    "    pid_cnt_dic                      =  df['pid'].value_counts().to_dict()\n",
    "    df_fea['pid_cnt']                =  df_fea['pid'].map(pid_cnt_dic).values\n",
    "    \n",
    "    # pid (od/d/o) :nunique, cnt/nunique\n",
    "    df['od']                         =  df['o'] + '_' + df['d']\n",
    "    pid_od_nunique                   =  df.groupby('pid')['od'].nunique().to_dict()\n",
    "    df_fea['pid_od_nunique']         =  df_fea['pid'].map(pid_od_nunique).values\n",
    "    \n",
    "    pid_o_nunique                    =  df.groupby('pid')['o'].nunique().to_dict()\n",
    "    df_fea['pid_o_nunique']          =  df_fea['pid'].map(pid_o_nunique).values\n",
    "    \n",
    "    pid_d_nunique                    =  df.groupby('pid')['d'].nunique().to_dict()\n",
    "    df_fea['pid_d_nunique']          =  df_fea['pid'].map(pid_d_nunique).values \n",
    "    return df_fea\n",
    " \n",
    "df_pid_queries_plans = get_pid_queries_plans(data)\n",
    "pid_queries_plans_features = [col for col in df_pid_queries_plans.columns if col not in ['sid','pid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于od的特征\n",
    "\n",
    "### 基于plans,queries表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_od_queries_plans(df):\n",
    "    df_fea                            =  pd.DataFrame() \n",
    "    df_fea['od']                      =  df['od'].unique()\n",
    "    df_fea['o']                       =  df_fea['od'].apply(lambda x: x.split('_')[0])\n",
    "    df_fea['d']                       =  df_fea['od'].apply(lambda x: x.split('_')[1])\n",
    "    \n",
    "    # ③基于o,d的特征 \n",
    "    # 2. o/d的编码信息,反应时间的热度等信息\n",
    "    # 2.1 cnt的编码信息,反应时间的热度  \n",
    "    # 2.2 cnt的百分比特征\n",
    "    df_fea['o_cnt']                    =  df_fea['o'].map(df['o'].value_counts())\n",
    "    \n",
    "    df_fea['d_cnt']                    =  df_fea['d'].map(df['d'].value_counts()) \n",
    "    df_fea['od_cnt']                   =  df_fea['od'].map(df['od'].value_counts()) \n",
    "    df_fea['od_div_o_cnt_pct']         =  df_fea['od_cnt'] / df_fea['o_cnt'] \n",
    "    df_fea['od_div_d_cnt_pct']         =  df_fea['od_cnt'] / df_fea['d_cnt']  \n",
    " \n",
    "    del df_fea['o']\n",
    "    del df_fea['d']\n",
    "    return df_fea\n",
    "    \n",
    "df_od_queries_plans = get_od_queries_plans(data)\n",
    "od_queries_plans_features = [col for col in df_od_queries_plans.columns if col not in ['sid','pid','o', 'd','od']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec特征（捕捉o&d的联系）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "outputing...\n",
      "5649\n",
      "5101\n"
     ]
    }
   ],
   "source": [
    "# 预处理复赛数据\n",
    "import os \n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence \n",
    "import multiprocessing \n",
    "\n",
    "L = 10\n",
    "save_path = './w2v'\n",
    "if not os.path.exists(save_path):\n",
    "    print(save_path)\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "sentence = data[['o','d']].astype(str).values\n",
    "sentence = sentence.tolist()\n",
    "print('training...')\n",
    "model = Word2Vec(sentence, size=L, window=20, min_count=1, workers=multiprocessing.cpu_count(),iter=10)\n",
    "print('outputing...')\n",
    "\n",
    "for fea in ['o','d']:\n",
    "    values = data[fea].unique()\n",
    "    print(len(values))\n",
    "    w2v = []\n",
    "    for i in values:\n",
    "        a = [i]\n",
    "        a.extend(model[str(i)])\n",
    "        w2v.append(a)\n",
    "    out_df = pd.DataFrame(w2v)\n",
    "\n",
    "    name = [fea]\n",
    "    for i in range(L):\n",
    "        name.append(name[0] + 'W' + str(i))\n",
    "    out_df.columns = name\n",
    "    out_df.to_csv(save_path + '/' + fea + '.csv', index=False)\n",
    "    \n",
    "w2v_path = './w2v/'\n",
    "try:\n",
    "    data = data.drop(w2v_features, axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594358\n",
      "594358\n",
      "594358\n",
      "594358\n",
      "['oW0', 'oW1', 'oW2', 'oW3', 'oW4', 'oW5', 'oW6', 'oW7', 'oW8', 'oW9', 'dW0', 'dW1', 'dW2', 'dW3', 'dW4', 'dW5', 'dW6', 'dW7', 'dW8', 'dW9']\n"
     ]
    }
   ],
   "source": [
    "w2v_features = []\n",
    "df_w2v = None\n",
    "for col in ['o','d']:\n",
    "    df = pd.read_csv(w2v_path + col + '.csv') \n",
    "    df = df.drop_duplicates([col])\n",
    "    fs = list(df)\n",
    "    fs.remove(col)\n",
    "    w2v_features += fs\n",
    "    print(len(data))\n",
    "    data_main = pd.merge(data_main, df, on=col, how='left')\n",
    "    print(len(data))\n",
    "print(w2v_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他组合特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pidod_queries_plans(df): \n",
    "    df_pid_fea = df[['sid','pid','req_time','o','d']].copy() \n",
    "    df_pid_fea['pid_o']       =  df['pid'].astype(str) + '_' + df['o'].astype(str)\n",
    "    df_pid_fea['pid_d']       =  df['pid'].astype(str) + '_' + df['d'].astype(str)\n",
    "    df_pid_fea['pid_od']      =  df_pid_fea['pid_o']   + '_' + df['d'].astype(str) \n",
    "    \n",
    "    df['pid_o']                               =  df['pid'].astype(str) + '_' + df['o'].astype(str)\n",
    "    df_pid_fea['pid_o_cnt']                   =  df_pid_fea['pid_o'].map(df['pid_o'].value_counts())\n",
    "    df['pid_d']                               =  df['pid'].astype(str) + '_' + df['d'].astype(str)\n",
    "    df_pid_fea['pid_d_cnt']                   =  df_pid_fea['pid_d'].map(df['pid_d'].value_counts())\n",
    "    df['pid_od']                              =  df['pid'].astype(str) + '_' + df['o'].astype(str)+ '_' + df['d'].astype(str)\n",
    "    df_pid_fea['pid_od_cnt']                  =  df_pid_fea['pid_od'].map(df['pid_od'].value_counts()) \n",
    "    \n",
    "    df_pid_fea['pid_od_div_pid_o_cnt']        =  df_pid_fea['pid_od_cnt'] / df_pid_fea['pid_o_cnt']\n",
    "    df_pid_fea['pid_od_div_pid_d_cnt']        =  df_pid_fea['pid_od_cnt'] / df_pid_fea['pid_d_cnt']\n",
    " \n",
    "    del df_pid_fea['o']\n",
    "    del df_pid_fea['d']\n",
    "    del df_pid_fea['pid_o']\n",
    "    del df_pid_fea['pid_d']\n",
    "    del df_pid_fea['pid_od']\n",
    "    del df_pid_fea['req_time']\n",
    "    del df_pid_fea['pid'] \n",
    "    return df_pid_fea\n",
    " \n",
    "df_pidod_queries_plans      = get_pidod_queries_plans(data)\n",
    "pidod_queries_plans_features = [col for col in df_pidod_queries_plans.columns if col not in ['sid','pid','req_time','o','d','pid_od_sym']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入profile文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "\n",
    "profile_decom_feature = []\n",
    "df_profile_decom = None\n",
    "for decom_func in [TruncatedSVD, NMF, PCA]:\n",
    "    x = profiles.drop(['pid'], axis=1).values\n",
    "    decom = decom_func(n_components=6, random_state=2018)\n",
    "    decom_x = decom.fit_transform(x)\n",
    "    decom_feas = pd.DataFrame(decom_x)\n",
    "    decom_feas.columns = ['{}_fea_{}'.format(decom_func.__name__, i) for i in range(6)]\n",
    "    profile_decom_feature += ['{}_fea_{}'.format(decom_func.__name__, i) for i in range(6)]\n",
    "    decom_feas['pid'] = profiles['pid'].values\n",
    "    if df_profile_decom is None:\n",
    "        df_profile_decom = decom_feas\n",
    "    else:\n",
    "        df_profile_decom = df_profile_decom.merge(decom_feas, 'left', 'pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TruncatedSVD_fea_0',\n",
       " 'TruncatedSVD_fea_1',\n",
       " 'TruncatedSVD_fea_2',\n",
       " 'TruncatedSVD_fea_3',\n",
       " 'TruncatedSVD_fea_4',\n",
       " 'TruncatedSVD_fea_5',\n",
       " 'NMF_fea_0',\n",
       " 'NMF_fea_1',\n",
       " 'NMF_fea_2',\n",
       " 'NMF_fea_3',\n",
       " 'NMF_fea_4',\n",
       " 'NMF_fea_5',\n",
       " 'PCA_fea_0',\n",
       " 'PCA_fea_1',\n",
       " 'PCA_fea_2',\n",
       " 'PCA_fea_3',\n",
       " 'PCA_fea_4',\n",
       " 'PCA_fea_5']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_decom_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转cate\n",
    "- 维度太高,不建议转cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 102.59it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 145.13it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1211.11it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 121.73it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 69.73it/s]\n"
     ]
    }
   ],
   "source": [
    "cate_cols = []\n",
    "for col in tqdm(df_sid_plans_pivot1.columns):\n",
    "    if (df_sid_plans_pivot1[col].nunique() <= 20) and ('_cnt' not in col) and (df_sid_plans_pivot1[col].nunique() > 2):\n",
    "        cate_cols.append(col)\n",
    "        \n",
    "for col in tqdm(df_sid_queries_plans.columns):\n",
    "    if (df_sid_queries_plans[col].nunique() <= 20) and ('_cnt' not in col) and (df_sid_queries_plans[col].nunique() > 2):\n",
    "        cate_cols.append(col)\n",
    "for col in tqdm(df_pid_queries_plans.columns):\n",
    "    if (df_pid_queries_plans[col].nunique() <= 20) and ('_cnt' not in col) and (df_pid_queries_plans[col].nunique() > 2):\n",
    "        cate_cols.append(col)\n",
    "        \n",
    "for col in tqdm(df_pidod_queries_plans.columns):\n",
    "    if (df_pidod_queries_plans[col].nunique() <= 20) and ('_cnt' not in col) and (df_pidod_queries_plans[col].nunique() > 2):\n",
    "        cate_cols.append(col)\n",
    "        \n",
    "for col in tqdm(df_od_queries_plans.columns):\n",
    "    if (df_od_queries_plans[col].nunique() <= 20) and ('_cnt' not in col) and (df_od_queries_plans[col].nunique() > 2):\n",
    "        cate_cols.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线下&线上验证  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels,preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "print(len(feature), feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入权重,线下验证:0.7上下波动\n",
    "- 调节不同类的样本会对我们的目标函数影响较大,这边的波动主要是w2v的引起的,w2v不控制训练的随机种子,每次生成的都不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 ['pid', 'sid_transport_cnt', 'sid_rank0_transport_mode', 'sid_rank0_price', 'sid_rank0_eta', 'sid_rank0_distance', 'sid_rank1_transport_mode', 'sid_rank1_price', 'sid_rank1_eta', 'sid_rank1_distance', 'sid_rank2_transport_mode', 'sid_rank2_price', 'sid_rank2_eta', 'sid_rank2_distance', 'sid_rank3_transport_mode', 'sid_rank3_price', 'sid_rank3_eta', 'sid_rank3_distance', 'sid_rank4_transport_mode', 'sid_rank4_price', 'sid_rank4_eta', 'sid_rank4_distance', 'sid_rank5_transport_mode', 'sid_rank5_price', 'sid_rank5_eta', 'sid_rank5_distance', 'sid_rank6_transport_mode', 'sid_rank6_price', 'sid_rank6_eta', 'sid_rank6_distance', 'sid_rank0_1_price', 'sid_rank0_1_eta', 'sid_rank0_1_distance', 'sid_rank1_2_price', 'sid_rank1_2_eta', 'sid_rank1_2_distance', 'sid_rank2_3_price', 'sid_rank2_3_eta', 'sid_rank2_3_distance', 'sid_rank3_4_price', 'sid_rank3_4_eta', 'sid_rank3_4_distance', 'sid_rank4_5_price', 'sid_rank4_5_eta', 'sid_rank4_5_distance', 'sid_price_div_distance_std', 'sid_etas_div_distance_std', 'sid_etas_div_price_std', 'sid_price_div_distance_max_min', 'sid_etas_div_price_max_min', 'sid_etas_div_distance_max_min', 'sid_distance_max_min', 'sid_price_max_min', 'sid_etas_max_min', 'sid_etas_mean', 'sid_price_mean', 'sid_distance_mean', 'sid_min_distance_transport_mode', 'sid_min_price_transport_mode', 'sid_min_etas_transport_mode', 'most_likely_mode', 'second_most_likely_mode', 'plan_time_minus_req_time', 'dow', 'hour', 'o_lng', 'o_lat', 'd_lng', 'd_lat', 'lng_diff', 'lat_diff', 'o_lbe', 'd_lbe', 'od_lbe', 'pid_cnt', 'pid_od_nunique', 'pid_o_nunique', 'pid_d_nunique', 'o_cnt', 'd_cnt', 'od_cnt', 'od_div_o_cnt_pct', 'od_div_d_cnt_pct', 'oW0', 'oW1', 'oW2', 'oW3', 'oW4', 'oW5', 'oW6', 'oW7', 'oW8', 'oW9', 'dW0', 'dW1', 'dW2', 'dW3', 'dW4', 'dW5', 'dW6', 'dW7', 'dW8', 'dW9', 'pidW0', 'pidW1', 'pidW2', 'pidW3', 'pidW4', 'pidW5', 'pidW6', 'pidW7', 'pidW8', 'pidW9', 'odW0', 'odW1', 'odW2', 'odW3', 'odW4', 'odW5', 'odW6', 'odW7', 'odW8', 'odW9', 'pid_o_cnt', 'pid_d_cnt', 'pid_od_cnt', 'pid_od_div_pid_o_cnt', 'pid_od_div_pid_d_cnt', 'TruncatedSVD_fea_0', 'TruncatedSVD_fea_1', 'TruncatedSVD_fea_2', 'TruncatedSVD_fea_3', 'TruncatedSVD_fea_4', 'TruncatedSVD_fea_5', 'NMF_fea_0', 'NMF_fea_1', 'NMF_fea_2', 'NMF_fea_3', 'NMF_fea_4', 'NMF_fea_5', 'PCA_fea_0', 'PCA_fea_1', 'PCA_fea_2', 'PCA_fea_3', 'PCA_fea_4', 'PCA_fea_5', 'p0', 'p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25', 'p26', 'p27', 'p28', 'p29', 'p30', 'p31', 'p32', 'p33', 'p34', 'p35', 'p36', 'p37', 'p38', 'p39', 'p40', 'p41', 'p42', 'p43', 'p44', 'p45', 'p46', 'p47', 'p48', 'p49', 'p50', 'p51', 'p52', 'p53', 'p54', 'p55', 'p56', 'p57', 'p58', 'p59', 'p60', 'p61', 'p62', 'p63', 'p64', 'p65']\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.644687\n",
      "[20]\tvalid_0's f1_weighted: 0.694915\n",
      "[30]\tvalid_0's f1_weighted: 0.697193\n",
      "[40]\tvalid_0's f1_weighted: 0.698205\n",
      "[50]\tvalid_0's f1_weighted: 0.698501\n",
      "[60]\tvalid_0's f1_weighted: 0.698865\n",
      "[70]\tvalid_0's f1_weighted: 0.699148\n",
      "[80]\tvalid_0's f1_weighted: 0.699291\n",
      "[90]\tvalid_0's f1_weighted: 0.699262\n",
      "[100]\tvalid_0's f1_weighted: 0.699277\n",
      "[110]\tvalid_0's f1_weighted: 0.699294\n",
      "[120]\tvalid_0's f1_weighted: 0.699384\n",
      "[130]\tvalid_0's f1_weighted: 0.699601\n",
      "[140]\tvalid_0's f1_weighted: 0.699429\n",
      "[150]\tvalid_0's f1_weighted: 0.699461\n",
      "[160]\tvalid_0's f1_weighted: 0.699626\n",
      "[170]\tvalid_0's f1_weighted: 0.699355\n",
      "[180]\tvalid_0's f1_weighted: 0.698993\n",
      "[190]\tvalid_0's f1_weighted: 0.699153\n",
      "[200]\tvalid_0's f1_weighted: 0.699327\n",
      "[210]\tvalid_0's f1_weighted: 0.699354\n",
      "[220]\tvalid_0's f1_weighted: 0.699189\n",
      "[230]\tvalid_0's f1_weighted: 0.698991\n",
      "[240]\tvalid_0's f1_weighted: 0.699006\n",
      "[250]\tvalid_0's f1_weighted: 0.699009\n",
      "[260]\tvalid_0's f1_weighted: 0.698812\n",
      "[270]\tvalid_0's f1_weighted: 0.699048\n",
      "[280]\tvalid_0's f1_weighted: 0.698993\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's f1_weighted: 0.699642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt',\n",
       "        class_weight={0: 1.05, 1: 0.75, 2: 0.75, 3: 1.75, 4: 2.7, 5: 0.65, 6: 1.55, 7: 0.65, 8: 1.3, 9: 1.05, 10: 1.1, 11: 1.25},\n",
       "        colsample_bytree=0.85, importance_type='split', learning_rate=0.05,\n",
       "        max_depth=-1, metric='None', min_child_samples=50,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=2000,\n",
       "        n_jobs=-1, num_leaves=100, objective='multiclass',\n",
       "        random_state=2019, reg_alpha=0, reg_lambda=0.01, silent=True,\n",
       "        subsample=0.9, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_features = [col for col in profiles if col not in ['pid']]\n",
    "data_ = data_main.merge(df_sid_plans_pivot1,    on ='sid', how='left')  \n",
    "data_ = data_.merge(df_sid_queries_plans,       on ='sid', how='left')  \n",
    "data_ = data_.merge(df_pid_queries_plans,       on ='pid', how='left') \n",
    "data_ = data_.merge(df_pidod_queries_plans,     on ='sid', how='left')    \n",
    "data_ = data_.merge(df_od_queries_plans,        on ='od', how='left')  \n",
    "data_ = data_.merge(df_profile_decom,           on ='pid', how='left')  \n",
    "data_ = data_.merge(profiles,                   on ='pid', how='left')  \n",
    "\n",
    "feature = main_features +  sid_plans_pivot_features1+ sid_queries_plans_features + pid_queries_plans_features + od_queries_plans_features + w2v_features +  pidod_queries_plans_features + profile_decom_feature+ profile_features  \n",
    "feature = [col for col in feature if col not in ['hour_minutes']]\n",
    "dic = dict(zip(validation_data['sid'].values, validation_data['click_mode'].values)) \n",
    "\n",
    "train_index = (data_.req_time < '2018-11-23')\n",
    "train_x     = data_[train_index][feature].fillna(-1).reset_index(drop=True) \n",
    "train_y     = data_[train_index].click_mode.reset_index(drop=True)\n",
    "\n",
    "valid_index = (data_.req_time > '2018-11-23') & (data.req_time < '2018-12-01')\n",
    "valid_x     = data_[valid_index][feature].fillna(-1).reset_index(drop=True)\n",
    "valid_y     = data_[valid_index].sid.map(dic).values \n",
    "weight = [1.05, 0.75, 0.75, 1.75, 2.7, 0.65, 1.55, 0.65, 1.3, 1.05, 1.1, 1.25]\n",
    "def f1_weighted(labels,preds):\n",
    "    preds = np.argmax(preds.reshape(12, -1), axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds, average='weighted')\n",
    "    return 'f1_weighted', score, True\n",
    "print(len(feature), feature)\n",
    "\n",
    "lgb_model1 = lgb.LGBMClassifier(boosting_type=\"gbdt\", num_leaves=100, reg_alpha=0, reg_lambda=0.01,\n",
    "    max_depth=-1, n_estimators=2000, objective='multiclass',class_weight=dict(zip(range(12),weight)), \n",
    "    subsample=0.9, colsample_bytree=0.85, subsample_freq=1, min_child_samples = 50,\n",
    "    learning_rate=0.05, random_state=2019, metric=\"None\")\n",
    "\n",
    "eval_set = [(valid_x, valid_y)]\n",
    "lgb_model1.fit(train_x, train_y, eval_set=eval_set, eval_metric=f1_weighted, verbose=10, early_stopping_rounds=150, categorical_feature=cate_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线上提交:0.7018左右波动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.623222\n",
      "[20]\tvalid_0's f1_weighted: 0.682952\n",
      "[30]\tvalid_0's f1_weighted: 0.684553\n",
      "[40]\tvalid_0's f1_weighted: 0.68506\n",
      "[50]\tvalid_0's f1_weighted: 0.685362\n",
      "[60]\tvalid_0's f1_weighted: 0.685788\n",
      "[70]\tvalid_0's f1_weighted: 0.68587\n",
      "[80]\tvalid_0's f1_weighted: 0.686199\n",
      "[90]\tvalid_0's f1_weighted: 0.686463\n",
      "[100]\tvalid_0's f1_weighted: 0.686851\n",
      "[110]\tvalid_0's f1_weighted: 0.687008\n",
      "[120]\tvalid_0's f1_weighted: 0.687123\n",
      "[130]\tvalid_0's f1_weighted: 0.687225\n",
      "[140]\tvalid_0's f1_weighted: 0.687388\n",
      "[150]\tvalid_0's f1_weighted: 0.687297\n",
      "[160]\tvalid_0's f1_weighted: 0.687282\n",
      "[170]\tvalid_0's f1_weighted: 0.687227\n",
      "[180]\tvalid_0's f1_weighted: 0.687114\n",
      "[190]\tvalid_0's f1_weighted: 0.687351\n",
      "[200]\tvalid_0's f1_weighted: 0.687413\n",
      "[210]\tvalid_0's f1_weighted: 0.687203\n",
      "[220]\tvalid_0's f1_weighted: 0.686898\n",
      "[230]\tvalid_0's f1_weighted: 0.687011\n",
      "[240]\tvalid_0's f1_weighted: 0.687394\n",
      "[250]\tvalid_0's f1_weighted: 0.687412\n",
      "[260]\tvalid_0's f1_weighted: 0.687181\n",
      "[270]\tvalid_0's f1_weighted: 0.687556\n",
      "[280]\tvalid_0's f1_weighted: 0.687821\n",
      "[290]\tvalid_0's f1_weighted: 0.687503\n",
      "[300]\tvalid_0's f1_weighted: 0.687648\n",
      "[310]\tvalid_0's f1_weighted: 0.687699\n",
      "[320]\tvalid_0's f1_weighted: 0.688261\n",
      "[330]\tvalid_0's f1_weighted: 0.688578\n",
      "[340]\tvalid_0's f1_weighted: 0.688482\n",
      "[350]\tvalid_0's f1_weighted: 0.688494\n",
      "[360]\tvalid_0's f1_weighted: 0.688777\n",
      "[370]\tvalid_0's f1_weighted: 0.688614\n",
      "[380]\tvalid_0's f1_weighted: 0.688455\n",
      "[390]\tvalid_0's f1_weighted: 0.68808\n",
      "[400]\tvalid_0's f1_weighted: 0.688205\n",
      "[410]\tvalid_0's f1_weighted: 0.68846\n",
      "[420]\tvalid_0's f1_weighted: 0.688284\n",
      "[430]\tvalid_0's f1_weighted: 0.688672\n",
      "[440]\tvalid_0's f1_weighted: 0.688625\n",
      "[450]\tvalid_0's f1_weighted: 0.688514\n",
      "[460]\tvalid_0's f1_weighted: 0.688484\n",
      "[470]\tvalid_0's f1_weighted: 0.688439\n",
      "[480]\tvalid_0's f1_weighted: 0.688268\n",
      "[490]\tvalid_0's f1_weighted: 0.688255\n",
      "[500]\tvalid_0's f1_weighted: 0.68828\n",
      "[510]\tvalid_0's f1_weighted: 0.688381\n",
      "Early stopping, best iteration is:\n",
      "[365]\tvalid_0's f1_weighted: 0.688897\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.617674\n",
      "[20]\tvalid_0's f1_weighted: 0.676808\n",
      "[30]\tvalid_0's f1_weighted: 0.679091\n",
      "[40]\tvalid_0's f1_weighted: 0.679821\n",
      "[50]\tvalid_0's f1_weighted: 0.679851\n",
      "[60]\tvalid_0's f1_weighted: 0.679878\n",
      "[70]\tvalid_0's f1_weighted: 0.679909\n",
      "[80]\tvalid_0's f1_weighted: 0.679892\n",
      "[90]\tvalid_0's f1_weighted: 0.680056\n",
      "[100]\tvalid_0's f1_weighted: 0.679926\n",
      "[110]\tvalid_0's f1_weighted: 0.680354\n",
      "[120]\tvalid_0's f1_weighted: 0.680563\n",
      "[130]\tvalid_0's f1_weighted: 0.680691\n",
      "[140]\tvalid_0's f1_weighted: 0.680981\n",
      "[150]\tvalid_0's f1_weighted: 0.681069\n",
      "[160]\tvalid_0's f1_weighted: 0.680924\n",
      "[170]\tvalid_0's f1_weighted: 0.681018\n",
      "[180]\tvalid_0's f1_weighted: 0.681374\n",
      "[190]\tvalid_0's f1_weighted: 0.681439\n",
      "[200]\tvalid_0's f1_weighted: 0.681521\n",
      "[210]\tvalid_0's f1_weighted: 0.68146\n",
      "[220]\tvalid_0's f1_weighted: 0.681552\n",
      "[230]\tvalid_0's f1_weighted: 0.681488\n",
      "[240]\tvalid_0's f1_weighted: 0.681436\n",
      "[250]\tvalid_0's f1_weighted: 0.681365\n",
      "[260]\tvalid_0's f1_weighted: 0.681328\n",
      "[270]\tvalid_0's f1_weighted: 0.681485\n",
      "[280]\tvalid_0's f1_weighted: 0.681766\n",
      "[290]\tvalid_0's f1_weighted: 0.681715\n",
      "[300]\tvalid_0's f1_weighted: 0.681964\n",
      "[310]\tvalid_0's f1_weighted: 0.682142\n",
      "[320]\tvalid_0's f1_weighted: 0.682233\n",
      "[330]\tvalid_0's f1_weighted: 0.682248\n",
      "[340]\tvalid_0's f1_weighted: 0.682474\n",
      "[350]\tvalid_0's f1_weighted: 0.682635\n",
      "[360]\tvalid_0's f1_weighted: 0.682528\n",
      "[370]\tvalid_0's f1_weighted: 0.682529\n",
      "[380]\tvalid_0's f1_weighted: 0.682512\n",
      "[390]\tvalid_0's f1_weighted: 0.682487\n",
      "[400]\tvalid_0's f1_weighted: 0.682312\n",
      "[410]\tvalid_0's f1_weighted: 0.68241\n",
      "[420]\tvalid_0's f1_weighted: 0.682378\n",
      "[430]\tvalid_0's f1_weighted: 0.682682\n",
      "[440]\tvalid_0's f1_weighted: 0.682504\n",
      "[450]\tvalid_0's f1_weighted: 0.682594\n",
      "[460]\tvalid_0's f1_weighted: 0.68274\n",
      "[470]\tvalid_0's f1_weighted: 0.682818\n",
      "[480]\tvalid_0's f1_weighted: 0.682761\n",
      "[490]\tvalid_0's f1_weighted: 0.682885\n",
      "[500]\tvalid_0's f1_weighted: 0.682588\n",
      "[510]\tvalid_0's f1_weighted: 0.682557\n",
      "[520]\tvalid_0's f1_weighted: 0.682825\n",
      "[530]\tvalid_0's f1_weighted: 0.682969\n",
      "[540]\tvalid_0's f1_weighted: 0.683034\n",
      "[550]\tvalid_0's f1_weighted: 0.682944\n",
      "[560]\tvalid_0's f1_weighted: 0.683349\n",
      "[570]\tvalid_0's f1_weighted: 0.683049\n",
      "[580]\tvalid_0's f1_weighted: 0.683405\n",
      "[590]\tvalid_0's f1_weighted: 0.683128\n",
      "[600]\tvalid_0's f1_weighted: 0.683136\n",
      "[610]\tvalid_0's f1_weighted: 0.683237\n",
      "[620]\tvalid_0's f1_weighted: 0.683263\n",
      "[630]\tvalid_0's f1_weighted: 0.683494\n",
      "[640]\tvalid_0's f1_weighted: 0.683444\n",
      "[650]\tvalid_0's f1_weighted: 0.683513\n",
      "[660]\tvalid_0's f1_weighted: 0.683577\n",
      "[670]\tvalid_0's f1_weighted: 0.683521\n",
      "[680]\tvalid_0's f1_weighted: 0.683439\n",
      "[690]\tvalid_0's f1_weighted: 0.683276\n",
      "[700]\tvalid_0's f1_weighted: 0.683367\n",
      "[710]\tvalid_0's f1_weighted: 0.683361\n",
      "[720]\tvalid_0's f1_weighted: 0.683417\n",
      "[730]\tvalid_0's f1_weighted: 0.683286\n",
      "[740]\tvalid_0's f1_weighted: 0.683208\n",
      "[750]\tvalid_0's f1_weighted: 0.683014\n",
      "[760]\tvalid_0's f1_weighted: 0.68331\n",
      "[770]\tvalid_0's f1_weighted: 0.683429\n",
      "[780]\tvalid_0's f1_weighted: 0.683365\n",
      "[790]\tvalid_0's f1_weighted: 0.68316\n",
      "[800]\tvalid_0's f1_weighted: 0.683142\n",
      "Early stopping, best iteration is:\n",
      "[654]\tvalid_0's f1_weighted: 0.683757\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.627546\n",
      "[20]\tvalid_0's f1_weighted: 0.685357\n",
      "[30]\tvalid_0's f1_weighted: 0.687996\n",
      "[40]\tvalid_0's f1_weighted: 0.688122\n",
      "[50]\tvalid_0's f1_weighted: 0.688216\n",
      "[60]\tvalid_0's f1_weighted: 0.688548\n",
      "[70]\tvalid_0's f1_weighted: 0.688821\n",
      "[80]\tvalid_0's f1_weighted: 0.689207\n",
      "[90]\tvalid_0's f1_weighted: 0.68899\n",
      "[100]\tvalid_0's f1_weighted: 0.689298\n",
      "[110]\tvalid_0's f1_weighted: 0.68924\n",
      "[120]\tvalid_0's f1_weighted: 0.689251\n",
      "[130]\tvalid_0's f1_weighted: 0.689273\n",
      "[140]\tvalid_0's f1_weighted: 0.689025\n",
      "[150]\tvalid_0's f1_weighted: 0.688744\n",
      "[160]\tvalid_0's f1_weighted: 0.689036\n",
      "[170]\tvalid_0's f1_weighted: 0.688742\n",
      "[180]\tvalid_0's f1_weighted: 0.688703\n",
      "[190]\tvalid_0's f1_weighted: 0.68898\n",
      "[200]\tvalid_0's f1_weighted: 0.688819\n",
      "[210]\tvalid_0's f1_weighted: 0.689074\n",
      "[220]\tvalid_0's f1_weighted: 0.688945\n",
      "[230]\tvalid_0's f1_weighted: 0.688891\n",
      "[240]\tvalid_0's f1_weighted: 0.689282\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's f1_weighted: 0.689377\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.622393\n",
      "[20]\tvalid_0's f1_weighted: 0.681105\n",
      "[30]\tvalid_0's f1_weighted: 0.683703\n",
      "[40]\tvalid_0's f1_weighted: 0.684158\n",
      "[50]\tvalid_0's f1_weighted: 0.684643\n",
      "[60]\tvalid_0's f1_weighted: 0.684989\n",
      "[70]\tvalid_0's f1_weighted: 0.685419\n",
      "[80]\tvalid_0's f1_weighted: 0.685357\n",
      "[90]\tvalid_0's f1_weighted: 0.685266\n",
      "[100]\tvalid_0's f1_weighted: 0.685171\n",
      "[110]\tvalid_0's f1_weighted: 0.684987\n",
      "[120]\tvalid_0's f1_weighted: 0.685172\n",
      "[130]\tvalid_0's f1_weighted: 0.685207\n",
      "[140]\tvalid_0's f1_weighted: 0.685488\n",
      "[150]\tvalid_0's f1_weighted: 0.685674\n",
      "[160]\tvalid_0's f1_weighted: 0.68549\n",
      "[170]\tvalid_0's f1_weighted: 0.685778\n",
      "[180]\tvalid_0's f1_weighted: 0.686017\n",
      "[190]\tvalid_0's f1_weighted: 0.685904\n",
      "[200]\tvalid_0's f1_weighted: 0.685955\n",
      "[210]\tvalid_0's f1_weighted: 0.686249\n",
      "[220]\tvalid_0's f1_weighted: 0.686205\n",
      "[230]\tvalid_0's f1_weighted: 0.686092\n",
      "[240]\tvalid_0's f1_weighted: 0.686253\n",
      "[250]\tvalid_0's f1_weighted: 0.686231\n",
      "[260]\tvalid_0's f1_weighted: 0.686339\n",
      "[270]\tvalid_0's f1_weighted: 0.686175\n",
      "[280]\tvalid_0's f1_weighted: 0.686287\n",
      "[290]\tvalid_0's f1_weighted: 0.686595\n",
      "[300]\tvalid_0's f1_weighted: 0.686809\n",
      "[310]\tvalid_0's f1_weighted: 0.686827\n",
      "[320]\tvalid_0's f1_weighted: 0.68645\n",
      "[330]\tvalid_0's f1_weighted: 0.686566\n",
      "[340]\tvalid_0's f1_weighted: 0.686557\n",
      "[350]\tvalid_0's f1_weighted: 0.686328\n",
      "[360]\tvalid_0's f1_weighted: 0.686399\n",
      "[370]\tvalid_0's f1_weighted: 0.686306\n",
      "[380]\tvalid_0's f1_weighted: 0.686454\n",
      "[390]\tvalid_0's f1_weighted: 0.686642\n",
      "[400]\tvalid_0's f1_weighted: 0.686811\n",
      "[410]\tvalid_0's f1_weighted: 0.68662\n",
      "[420]\tvalid_0's f1_weighted: 0.686662\n",
      "[430]\tvalid_0's f1_weighted: 0.686521\n",
      "[440]\tvalid_0's f1_weighted: 0.686623\n",
      "Early stopping, best iteration is:\n",
      "[299]\tvalid_0's f1_weighted: 0.686895\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's f1_weighted: 0.622115\n",
      "[20]\tvalid_0's f1_weighted: 0.682149\n",
      "[30]\tvalid_0's f1_weighted: 0.68398\n",
      "[40]\tvalid_0's f1_weighted: 0.684543\n",
      "[50]\tvalid_0's f1_weighted: 0.684782\n",
      "[60]\tvalid_0's f1_weighted: 0.685423\n",
      "[70]\tvalid_0's f1_weighted: 0.685517\n",
      "[80]\tvalid_0's f1_weighted: 0.685654\n",
      "[90]\tvalid_0's f1_weighted: 0.685762\n",
      "[100]\tvalid_0's f1_weighted: 0.685929\n",
      "[110]\tvalid_0's f1_weighted: 0.686033\n",
      "[120]\tvalid_0's f1_weighted: 0.685971\n",
      "[130]\tvalid_0's f1_weighted: 0.686264\n",
      "[140]\tvalid_0's f1_weighted: 0.686556\n",
      "[150]\tvalid_0's f1_weighted: 0.68678\n",
      "[160]\tvalid_0's f1_weighted: 0.686754\n",
      "[170]\tvalid_0's f1_weighted: 0.686941\n",
      "[180]\tvalid_0's f1_weighted: 0.686543\n",
      "[190]\tvalid_0's f1_weighted: 0.686861\n",
      "[200]\tvalid_0's f1_weighted: 0.686637\n",
      "[210]\tvalid_0's f1_weighted: 0.686531\n",
      "[220]\tvalid_0's f1_weighted: 0.686982\n",
      "[230]\tvalid_0's f1_weighted: 0.686897\n",
      "[240]\tvalid_0's f1_weighted: 0.686856\n",
      "[250]\tvalid_0's f1_weighted: 0.687016\n",
      "[260]\tvalid_0's f1_weighted: 0.686756\n",
      "[270]\tvalid_0's f1_weighted: 0.686987\n",
      "[280]\tvalid_0's f1_weighted: 0.687275\n",
      "[290]\tvalid_0's f1_weighted: 0.687106\n",
      "[300]\tvalid_0's f1_weighted: 0.687\n",
      "[310]\tvalid_0's f1_weighted: 0.687335\n",
      "[320]\tvalid_0's f1_weighted: 0.687564\n",
      "[330]\tvalid_0's f1_weighted: 0.687405\n",
      "[340]\tvalid_0's f1_weighted: 0.687642\n",
      "[350]\tvalid_0's f1_weighted: 0.687597\n",
      "[360]\tvalid_0's f1_weighted: 0.68798\n",
      "[370]\tvalid_0's f1_weighted: 0.687593\n",
      "[380]\tvalid_0's f1_weighted: 0.687452\n",
      "[390]\tvalid_0's f1_weighted: 0.687437\n",
      "[400]\tvalid_0's f1_weighted: 0.687215\n",
      "[410]\tvalid_0's f1_weighted: 0.687125\n",
      "[420]\tvalid_0's f1_weighted: 0.687134\n",
      "[430]\tvalid_0's f1_weighted: 0.687036\n",
      "[440]\tvalid_0's f1_weighted: 0.687243\n",
      "[450]\tvalid_0's f1_weighted: 0.687026\n",
      "[460]\tvalid_0's f1_weighted: 0.687141\n",
      "[470]\tvalid_0's f1_weighted: 0.687355\n",
      "[480]\tvalid_0's f1_weighted: 0.687606\n",
      "[490]\tvalid_0's f1_weighted: 0.687539\n",
      "[500]\tvalid_0's f1_weighted: 0.687474\n",
      "[510]\tvalid_0's f1_weighted: 0.687602\n",
      "Early stopping, best iteration is:\n",
      "[360]\tvalid_0's f1_weighted: 0.68798\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.622062\n",
      "[20]\tvalid_0's f1_weighted: 0.684074\n",
      "[30]\tvalid_0's f1_weighted: 0.686235\n",
      "[40]\tvalid_0's f1_weighted: 0.686546\n",
      "[50]\tvalid_0's f1_weighted: 0.686531\n",
      "[60]\tvalid_0's f1_weighted: 0.686943\n",
      "[70]\tvalid_0's f1_weighted: 0.687096\n",
      "[80]\tvalid_0's f1_weighted: 0.687224\n",
      "[90]\tvalid_0's f1_weighted: 0.687475\n",
      "[100]\tvalid_0's f1_weighted: 0.687339\n",
      "[110]\tvalid_0's f1_weighted: 0.687367\n",
      "[120]\tvalid_0's f1_weighted: 0.687419\n",
      "[130]\tvalid_0's f1_weighted: 0.687357\n",
      "[140]\tvalid_0's f1_weighted: 0.687356\n",
      "[150]\tvalid_0's f1_weighted: 0.687394\n",
      "[160]\tvalid_0's f1_weighted: 0.687301\n",
      "[170]\tvalid_0's f1_weighted: 0.687168\n",
      "[180]\tvalid_0's f1_weighted: 0.687145\n",
      "[190]\tvalid_0's f1_weighted: 0.687347\n",
      "[200]\tvalid_0's f1_weighted: 0.68761\n",
      "[210]\tvalid_0's f1_weighted: 0.687783\n",
      "[220]\tvalid_0's f1_weighted: 0.687746\n",
      "[230]\tvalid_0's f1_weighted: 0.687502\n",
      "[240]\tvalid_0's f1_weighted: 0.687661\n",
      "[250]\tvalid_0's f1_weighted: 0.68782\n",
      "[260]\tvalid_0's f1_weighted: 0.687777\n",
      "[270]\tvalid_0's f1_weighted: 0.687878\n",
      "[280]\tvalid_0's f1_weighted: 0.687984\n",
      "[290]\tvalid_0's f1_weighted: 0.68833\n",
      "[300]\tvalid_0's f1_weighted: 0.688327\n",
      "[310]\tvalid_0's f1_weighted: 0.688351\n",
      "[320]\tvalid_0's f1_weighted: 0.688186\n",
      "[330]\tvalid_0's f1_weighted: 0.687911\n",
      "[340]\tvalid_0's f1_weighted: 0.688172\n",
      "[350]\tvalid_0's f1_weighted: 0.688348\n",
      "[360]\tvalid_0's f1_weighted: 0.688352\n",
      "[370]\tvalid_0's f1_weighted: 0.688349\n",
      "[380]\tvalid_0's f1_weighted: 0.688526\n",
      "[390]\tvalid_0's f1_weighted: 0.688378\n",
      "[400]\tvalid_0's f1_weighted: 0.688223\n",
      "[410]\tvalid_0's f1_weighted: 0.688425\n",
      "[420]\tvalid_0's f1_weighted: 0.688661\n",
      "[430]\tvalid_0's f1_weighted: 0.689081\n",
      "[440]\tvalid_0's f1_weighted: 0.688755\n",
      "[450]\tvalid_0's f1_weighted: 0.688703\n",
      "[460]\tvalid_0's f1_weighted: 0.688588\n",
      "[470]\tvalid_0's f1_weighted: 0.688954\n",
      "[480]\tvalid_0's f1_weighted: 0.688934\n",
      "[490]\tvalid_0's f1_weighted: 0.689148\n",
      "[500]\tvalid_0's f1_weighted: 0.688945\n",
      "[510]\tvalid_0's f1_weighted: 0.689131\n",
      "[520]\tvalid_0's f1_weighted: 0.689329\n",
      "[530]\tvalid_0's f1_weighted: 0.689038\n",
      "[540]\tvalid_0's f1_weighted: 0.689145\n",
      "[550]\tvalid_0's f1_weighted: 0.689202\n",
      "[560]\tvalid_0's f1_weighted: 0.689062\n",
      "[570]\tvalid_0's f1_weighted: 0.689168\n",
      "[580]\tvalid_0's f1_weighted: 0.689317\n",
      "[590]\tvalid_0's f1_weighted: 0.689462\n",
      "[600]\tvalid_0's f1_weighted: 0.689591\n",
      "[610]\tvalid_0's f1_weighted: 0.689372\n",
      "[620]\tvalid_0's f1_weighted: 0.689336\n",
      "[630]\tvalid_0's f1_weighted: 0.689683\n",
      "[640]\tvalid_0's f1_weighted: 0.689621\n",
      "[650]\tvalid_0's f1_weighted: 0.689723\n",
      "[660]\tvalid_0's f1_weighted: 0.689682\n",
      "[670]\tvalid_0's f1_weighted: 0.689868\n",
      "[680]\tvalid_0's f1_weighted: 0.689911\n",
      "[690]\tvalid_0's f1_weighted: 0.689929\n",
      "[700]\tvalid_0's f1_weighted: 0.690069\n",
      "[710]\tvalid_0's f1_weighted: 0.68983\n",
      "[720]\tvalid_0's f1_weighted: 0.689914\n",
      "[730]\tvalid_0's f1_weighted: 0.689908\n",
      "[740]\tvalid_0's f1_weighted: 0.689853\n",
      "[750]\tvalid_0's f1_weighted: 0.689723\n",
      "[760]\tvalid_0's f1_weighted: 0.689713\n",
      "[770]\tvalid_0's f1_weighted: 0.689888\n",
      "[780]\tvalid_0's f1_weighted: 0.689614\n",
      "[790]\tvalid_0's f1_weighted: 0.689799\n",
      "[800]\tvalid_0's f1_weighted: 0.689691\n",
      "[810]\tvalid_0's f1_weighted: 0.689678\n",
      "[820]\tvalid_0's f1_weighted: 0.689804\n",
      "[830]\tvalid_0's f1_weighted: 0.689878\n",
      "[840]\tvalid_0's f1_weighted: 0.690235\n",
      "[850]\tvalid_0's f1_weighted: 0.690132\n",
      "[860]\tvalid_0's f1_weighted: 0.690304\n",
      "[870]\tvalid_0's f1_weighted: 0.689976\n",
      "[880]\tvalid_0's f1_weighted: 0.689995\n",
      "[890]\tvalid_0's f1_weighted: 0.69007\n",
      "[900]\tvalid_0's f1_weighted: 0.690098\n",
      "[910]\tvalid_0's f1_weighted: 0.690077\n",
      "[920]\tvalid_0's f1_weighted: 0.690191\n",
      "[930]\tvalid_0's f1_weighted: 0.69021\n",
      "[940]\tvalid_0's f1_weighted: 0.689924\n",
      "[950]\tvalid_0's f1_weighted: 0.689826\n",
      "[960]\tvalid_0's f1_weighted: 0.68976\n",
      "[970]\tvalid_0's f1_weighted: 0.689807\n",
      "[980]\tvalid_0's f1_weighted: 0.689875\n",
      "[990]\tvalid_0's f1_weighted: 0.68958\n",
      "[1000]\tvalid_0's f1_weighted: 0.689588\n",
      "[1010]\tvalid_0's f1_weighted: 0.689696\n",
      "[1020]\tvalid_0's f1_weighted: 0.689735\n",
      "[1030]\tvalid_0's f1_weighted: 0.68989\n",
      "[1040]\tvalid_0's f1_weighted: 0.689846\n",
      "[1050]\tvalid_0's f1_weighted: 0.689754\n",
      "[1060]\tvalid_0's f1_weighted: 0.689642\n",
      "[1070]\tvalid_0's f1_weighted: 0.689496\n",
      "Early stopping, best iteration is:\n",
      "[928]\tvalid_0's f1_weighted: 0.690323\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.623747\n",
      "[20]\tvalid_0's f1_weighted: 0.684135\n",
      "[30]\tvalid_0's f1_weighted: 0.686376\n",
      "[40]\tvalid_0's f1_weighted: 0.686873\n",
      "[50]\tvalid_0's f1_weighted: 0.687179\n",
      "[60]\tvalid_0's f1_weighted: 0.687681\n",
      "[70]\tvalid_0's f1_weighted: 0.687714\n",
      "[80]\tvalid_0's f1_weighted: 0.687768\n",
      "[90]\tvalid_0's f1_weighted: 0.687666\n",
      "[100]\tvalid_0's f1_weighted: 0.687769\n",
      "[110]\tvalid_0's f1_weighted: 0.687985\n",
      "[120]\tvalid_0's f1_weighted: 0.687976\n",
      "[130]\tvalid_0's f1_weighted: 0.688115\n",
      "[140]\tvalid_0's f1_weighted: 0.688021\n",
      "[150]\tvalid_0's f1_weighted: 0.688446\n",
      "[160]\tvalid_0's f1_weighted: 0.688558\n",
      "[170]\tvalid_0's f1_weighted: 0.68842\n",
      "[180]\tvalid_0's f1_weighted: 0.688768\n",
      "[190]\tvalid_0's f1_weighted: 0.688767\n",
      "[200]\tvalid_0's f1_weighted: 0.688844\n",
      "[210]\tvalid_0's f1_weighted: 0.688872\n",
      "[220]\tvalid_0's f1_weighted: 0.688848\n",
      "[230]\tvalid_0's f1_weighted: 0.689087\n",
      "[240]\tvalid_0's f1_weighted: 0.689108\n",
      "[250]\tvalid_0's f1_weighted: 0.689265\n",
      "[260]\tvalid_0's f1_weighted: 0.689552\n",
      "[270]\tvalid_0's f1_weighted: 0.689692\n",
      "[280]\tvalid_0's f1_weighted: 0.689426\n",
      "[290]\tvalid_0's f1_weighted: 0.689512\n",
      "[300]\tvalid_0's f1_weighted: 0.689586\n",
      "[310]\tvalid_0's f1_weighted: 0.689341\n",
      "[320]\tvalid_0's f1_weighted: 0.689333\n",
      "[330]\tvalid_0's f1_weighted: 0.689459\n",
      "[340]\tvalid_0's f1_weighted: 0.689543\n",
      "[350]\tvalid_0's f1_weighted: 0.689604\n",
      "[360]\tvalid_0's f1_weighted: 0.689646\n",
      "[370]\tvalid_0's f1_weighted: 0.689682\n",
      "[380]\tvalid_0's f1_weighted: 0.689572\n",
      "[390]\tvalid_0's f1_weighted: 0.689549\n",
      "[400]\tvalid_0's f1_weighted: 0.689334\n",
      "[410]\tvalid_0's f1_weighted: 0.689462\n",
      "[420]\tvalid_0's f1_weighted: 0.689548\n",
      "[430]\tvalid_0's f1_weighted: 0.689568\n",
      "[440]\tvalid_0's f1_weighted: 0.689754\n",
      "[450]\tvalid_0's f1_weighted: 0.689962\n",
      "[460]\tvalid_0's f1_weighted: 0.689997\n",
      "[470]\tvalid_0's f1_weighted: 0.689881\n",
      "[480]\tvalid_0's f1_weighted: 0.68977\n",
      "[490]\tvalid_0's f1_weighted: 0.689796\n",
      "[500]\tvalid_0's f1_weighted: 0.689814\n",
      "[510]\tvalid_0's f1_weighted: 0.689887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[520]\tvalid_0's f1_weighted: 0.689656\n",
      "[530]\tvalid_0's f1_weighted: 0.689649\n",
      "[540]\tvalid_0's f1_weighted: 0.689949\n",
      "[550]\tvalid_0's f1_weighted: 0.690007\n",
      "[560]\tvalid_0's f1_weighted: 0.689903\n",
      "[570]\tvalid_0's f1_weighted: 0.690062\n",
      "[580]\tvalid_0's f1_weighted: 0.690107\n",
      "[590]\tvalid_0's f1_weighted: 0.690177\n",
      "[600]\tvalid_0's f1_weighted: 0.690091\n",
      "[610]\tvalid_0's f1_weighted: 0.690148\n",
      "[620]\tvalid_0's f1_weighted: 0.690094\n",
      "[630]\tvalid_0's f1_weighted: 0.689874\n",
      "[640]\tvalid_0's f1_weighted: 0.690109\n",
      "[650]\tvalid_0's f1_weighted: 0.690294\n",
      "[660]\tvalid_0's f1_weighted: 0.690035\n",
      "[670]\tvalid_0's f1_weighted: 0.689907\n",
      "[680]\tvalid_0's f1_weighted: 0.690134\n",
      "[690]\tvalid_0's f1_weighted: 0.69021\n",
      "[700]\tvalid_0's f1_weighted: 0.690504\n",
      "[710]\tvalid_0's f1_weighted: 0.690587\n",
      "[720]\tvalid_0's f1_weighted: 0.690941\n",
      "[730]\tvalid_0's f1_weighted: 0.690665\n",
      "[740]\tvalid_0's f1_weighted: 0.690817\n",
      "[750]\tvalid_0's f1_weighted: 0.690749\n",
      "[760]\tvalid_0's f1_weighted: 0.690493\n",
      "[770]\tvalid_0's f1_weighted: 0.690563\n",
      "[780]\tvalid_0's f1_weighted: 0.690289\n",
      "[790]\tvalid_0's f1_weighted: 0.690534\n",
      "[800]\tvalid_0's f1_weighted: 0.690684\n",
      "[810]\tvalid_0's f1_weighted: 0.691061\n",
      "[820]\tvalid_0's f1_weighted: 0.6909\n",
      "[830]\tvalid_0's f1_weighted: 0.690707\n",
      "[840]\tvalid_0's f1_weighted: 0.690913\n",
      "[850]\tvalid_0's f1_weighted: 0.690943\n",
      "[860]\tvalid_0's f1_weighted: 0.690543\n",
      "[870]\tvalid_0's f1_weighted: 0.690702\n",
      "[880]\tvalid_0's f1_weighted: 0.690795\n",
      "[890]\tvalid_0's f1_weighted: 0.690626\n",
      "[900]\tvalid_0's f1_weighted: 0.690813\n",
      "[910]\tvalid_0's f1_weighted: 0.690803\n",
      "[920]\tvalid_0's f1_weighted: 0.690941\n",
      "[930]\tvalid_0's f1_weighted: 0.690819\n",
      "[940]\tvalid_0's f1_weighted: 0.690933\n",
      "[950]\tvalid_0's f1_weighted: 0.690822\n",
      "[960]\tvalid_0's f1_weighted: 0.690937\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's f1_weighted: 0.69113\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.620863\n",
      "[20]\tvalid_0's f1_weighted: 0.682562\n",
      "[30]\tvalid_0's f1_weighted: 0.684767\n",
      "[40]\tvalid_0's f1_weighted: 0.685641\n",
      "[50]\tvalid_0's f1_weighted: 0.686077\n",
      "[60]\tvalid_0's f1_weighted: 0.686321\n",
      "[70]\tvalid_0's f1_weighted: 0.686532\n",
      "[80]\tvalid_0's f1_weighted: 0.686831\n",
      "[90]\tvalid_0's f1_weighted: 0.686837\n",
      "[100]\tvalid_0's f1_weighted: 0.686749\n",
      "[110]\tvalid_0's f1_weighted: 0.687243\n",
      "[120]\tvalid_0's f1_weighted: 0.687292\n",
      "[130]\tvalid_0's f1_weighted: 0.68724\n",
      "[140]\tvalid_0's f1_weighted: 0.687313\n",
      "[150]\tvalid_0's f1_weighted: 0.687324\n",
      "[160]\tvalid_0's f1_weighted: 0.687416\n",
      "[170]\tvalid_0's f1_weighted: 0.687587\n",
      "[180]\tvalid_0's f1_weighted: 0.6876\n",
      "[190]\tvalid_0's f1_weighted: 0.687401\n",
      "[200]\tvalid_0's f1_weighted: 0.687365\n",
      "[210]\tvalid_0's f1_weighted: 0.687219\n",
      "[220]\tvalid_0's f1_weighted: 0.687405\n",
      "[230]\tvalid_0's f1_weighted: 0.687479\n",
      "[240]\tvalid_0's f1_weighted: 0.68752\n",
      "[250]\tvalid_0's f1_weighted: 0.68752\n",
      "[260]\tvalid_0's f1_weighted: 0.687714\n",
      "[270]\tvalid_0's f1_weighted: 0.687652\n",
      "[280]\tvalid_0's f1_weighted: 0.687643\n",
      "[290]\tvalid_0's f1_weighted: 0.68752\n",
      "[300]\tvalid_0's f1_weighted: 0.687295\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's f1_weighted: 0.687716\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.624087\n",
      "[20]\tvalid_0's f1_weighted: 0.683623\n",
      "[30]\tvalid_0's f1_weighted: 0.685664\n",
      "[40]\tvalid_0's f1_weighted: 0.685907\n",
      "[50]\tvalid_0's f1_weighted: 0.686433\n",
      "[60]\tvalid_0's f1_weighted: 0.686229\n",
      "[70]\tvalid_0's f1_weighted: 0.686651\n",
      "[80]\tvalid_0's f1_weighted: 0.686667\n",
      "[90]\tvalid_0's f1_weighted: 0.686948\n",
      "[100]\tvalid_0's f1_weighted: 0.686687\n",
      "[110]\tvalid_0's f1_weighted: 0.686482\n",
      "[120]\tvalid_0's f1_weighted: 0.686599\n",
      "[130]\tvalid_0's f1_weighted: 0.6868\n",
      "[140]\tvalid_0's f1_weighted: 0.687152\n",
      "[150]\tvalid_0's f1_weighted: 0.687424\n",
      "[160]\tvalid_0's f1_weighted: 0.687521\n",
      "[170]\tvalid_0's f1_weighted: 0.68768\n",
      "[180]\tvalid_0's f1_weighted: 0.687825\n",
      "[190]\tvalid_0's f1_weighted: 0.687522\n",
      "[200]\tvalid_0's f1_weighted: 0.687358\n",
      "[210]\tvalid_0's f1_weighted: 0.687692\n",
      "[220]\tvalid_0's f1_weighted: 0.687624\n",
      "[230]\tvalid_0's f1_weighted: 0.687665\n",
      "[240]\tvalid_0's f1_weighted: 0.687625\n",
      "[250]\tvalid_0's f1_weighted: 0.687837\n",
      "[260]\tvalid_0's f1_weighted: 0.687892\n",
      "[270]\tvalid_0's f1_weighted: 0.688413\n",
      "[280]\tvalid_0's f1_weighted: 0.688663\n",
      "[290]\tvalid_0's f1_weighted: 0.688588\n",
      "[300]\tvalid_0's f1_weighted: 0.688768\n",
      "[310]\tvalid_0's f1_weighted: 0.688806\n",
      "[320]\tvalid_0's f1_weighted: 0.688925\n",
      "[330]\tvalid_0's f1_weighted: 0.688994\n",
      "[340]\tvalid_0's f1_weighted: 0.689194\n",
      "[350]\tvalid_0's f1_weighted: 0.689102\n",
      "[360]\tvalid_0's f1_weighted: 0.688869\n",
      "[370]\tvalid_0's f1_weighted: 0.688903\n",
      "[380]\tvalid_0's f1_weighted: 0.689033\n",
      "[390]\tvalid_0's f1_weighted: 0.688883\n",
      "[400]\tvalid_0's f1_weighted: 0.689153\n",
      "[410]\tvalid_0's f1_weighted: 0.689232\n",
      "[420]\tvalid_0's f1_weighted: 0.68904\n",
      "[430]\tvalid_0's f1_weighted: 0.689224\n",
      "[440]\tvalid_0's f1_weighted: 0.689164\n",
      "[450]\tvalid_0's f1_weighted: 0.689283\n",
      "[460]\tvalid_0's f1_weighted: 0.689527\n",
      "[470]\tvalid_0's f1_weighted: 0.689218\n",
      "[480]\tvalid_0's f1_weighted: 0.689319\n",
      "[490]\tvalid_0's f1_weighted: 0.68942\n",
      "[500]\tvalid_0's f1_weighted: 0.689387\n",
      "[510]\tvalid_0's f1_weighted: 0.689301\n",
      "[520]\tvalid_0's f1_weighted: 0.689429\n",
      "[530]\tvalid_0's f1_weighted: 0.689519\n",
      "[540]\tvalid_0's f1_weighted: 0.689665\n",
      "[550]\tvalid_0's f1_weighted: 0.689452\n",
      "[560]\tvalid_0's f1_weighted: 0.689295\n",
      "[570]\tvalid_0's f1_weighted: 0.689256\n",
      "[580]\tvalid_0's f1_weighted: 0.689458\n",
      "[590]\tvalid_0's f1_weighted: 0.689578\n",
      "[600]\tvalid_0's f1_weighted: 0.689497\n",
      "[610]\tvalid_0's f1_weighted: 0.689612\n",
      "[620]\tvalid_0's f1_weighted: 0.689579\n",
      "[630]\tvalid_0's f1_weighted: 0.689452\n",
      "[640]\tvalid_0's f1_weighted: 0.689794\n",
      "[650]\tvalid_0's f1_weighted: 0.68943\n",
      "[660]\tvalid_0's f1_weighted: 0.689662\n",
      "[670]\tvalid_0's f1_weighted: 0.689535\n",
      "[680]\tvalid_0's f1_weighted: 0.689573\n",
      "[690]\tvalid_0's f1_weighted: 0.689809\n",
      "[700]\tvalid_0's f1_weighted: 0.689756\n",
      "[710]\tvalid_0's f1_weighted: 0.689768\n",
      "[720]\tvalid_0's f1_weighted: 0.689742\n",
      "[730]\tvalid_0's f1_weighted: 0.689847\n",
      "[740]\tvalid_0's f1_weighted: 0.689797\n",
      "[750]\tvalid_0's f1_weighted: 0.68986\n",
      "[760]\tvalid_0's f1_weighted: 0.689783\n",
      "[770]\tvalid_0's f1_weighted: 0.689605\n",
      "[780]\tvalid_0's f1_weighted: 0.689557\n",
      "[790]\tvalid_0's f1_weighted: 0.689258\n",
      "[800]\tvalid_0's f1_weighted: 0.689185\n",
      "[810]\tvalid_0's f1_weighted: 0.689197\n",
      "[820]\tvalid_0's f1_weighted: 0.689519\n",
      "[830]\tvalid_0's f1_weighted: 0.689441\n",
      "[840]\tvalid_0's f1_weighted: 0.689207\n",
      "[850]\tvalid_0's f1_weighted: 0.689085\n",
      "[860]\tvalid_0's f1_weighted: 0.689268\n",
      "[870]\tvalid_0's f1_weighted: 0.689399\n",
      "Early stopping, best iteration is:\n",
      "[724]\tvalid_0's f1_weighted: 0.689921\n",
      "FOLD: \n",
      "50000 450000\n",
      "Training until validation scores don't improve for 150 rounds.\n",
      "[10]\tvalid_0's f1_weighted: 0.615806\n",
      "[20]\tvalid_0's f1_weighted: 0.676831\n",
      "[30]\tvalid_0's f1_weighted: 0.679833\n",
      "[40]\tvalid_0's f1_weighted: 0.680705\n",
      "[50]\tvalid_0's f1_weighted: 0.681399\n",
      "[60]\tvalid_0's f1_weighted: 0.681609\n",
      "[70]\tvalid_0's f1_weighted: 0.681753\n",
      "[80]\tvalid_0's f1_weighted: 0.682079\n",
      "[90]\tvalid_0's f1_weighted: 0.682437\n",
      "[100]\tvalid_0's f1_weighted: 0.68265\n",
      "[110]\tvalid_0's f1_weighted: 0.68265\n",
      "[120]\tvalid_0's f1_weighted: 0.683109\n",
      "[130]\tvalid_0's f1_weighted: 0.683191\n",
      "[140]\tvalid_0's f1_weighted: 0.683283\n",
      "[150]\tvalid_0's f1_weighted: 0.682995\n",
      "[160]\tvalid_0's f1_weighted: 0.68322\n",
      "[170]\tvalid_0's f1_weighted: 0.683324\n",
      "[180]\tvalid_0's f1_weighted: 0.683303\n",
      "[190]\tvalid_0's f1_weighted: 0.68372\n",
      "[200]\tvalid_0's f1_weighted: 0.683549\n",
      "[210]\tvalid_0's f1_weighted: 0.683318\n",
      "[220]\tvalid_0's f1_weighted: 0.682978\n",
      "[230]\tvalid_0's f1_weighted: 0.683185\n",
      "[240]\tvalid_0's f1_weighted: 0.68326\n",
      "[250]\tvalid_0's f1_weighted: 0.683371\n",
      "[260]\tvalid_0's f1_weighted: 0.683274\n",
      "[270]\tvalid_0's f1_weighted: 0.683275\n",
      "[280]\tvalid_0's f1_weighted: 0.683304\n",
      "[290]\tvalid_0's f1_weighted: 0.683533\n",
      "[300]\tvalid_0's f1_weighted: 0.683536\n",
      "[310]\tvalid_0's f1_weighted: 0.683469\n",
      "[320]\tvalid_0's f1_weighted: 0.683566\n",
      "[330]\tvalid_0's f1_weighted: 0.683572\n",
      "[340]\tvalid_0's f1_weighted: 0.683618\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid_0's f1_weighted: 0.683748\n"
     ]
    }
   ],
   "source": [
    "train_index = (data_.req_time < '2018-12-01')\n",
    "train_x     = data_[train_index][feature].fillna(-1).reset_index(drop=True) \n",
    "train_y     = data_[train_index].click_mode.reset_index(drop=True) \n",
    "test_index = (data_.req_time >= '2018-12-01')\n",
    "test_x     = data_[test_index][feature].fillna(-1).reset_index(drop=True)\n",
    "\n",
    "i = 0\n",
    "meta_train = np.zeros(shape = ((len(train_x)),12))\n",
    "from sklearn.model_selection import StratifiedKFold,KFold \n",
    "\n",
    "spilts_num = 10\n",
    "skf = KFold(n_splits=spilts_num, shuffle=False)\n",
    "pred_test = 0\n",
    "pred_val  = 0\n",
    "models    = []\n",
    "\n",
    "weight = [1.05, 0.75, 0.75, 1.75, 2.7, 0.65, 1.55, 0.65, 1.3, 1.05, 1.1, 1.25]\n",
    "\n",
    "for tr_ind,te_ind in skf.split(train_y):\n",
    "    print('FOLD: '.format(i))\n",
    "    print(len(te_ind),len(tr_ind)) \n",
    "    X_train,X_train_label = train_x.iloc[tr_ind],train_y[tr_ind]\n",
    "    X_val,X_val_label     = train_x.iloc[te_ind],train_y[te_ind]\n",
    "    \n",
    "    model =  lgb.LGBMClassifier(boosting_type=\"gbdt\", num_leaves=100, reg_alpha=0, reg_lambda=0.01,\n",
    "            max_depth=-1, n_estimators=2000, objective='multiclass',\n",
    "            subsample=0.9, colsample_bytree=0.85, subsample_freq=1, min_child_samples = 50,class_weight=dict(zip(range(12),weight)),\n",
    "            learning_rate=0.05, random_state=2019, metric=\"None\")\n",
    "    models.append(model)\n",
    "    eval_set = [(X_val, X_val_label)]\n",
    "    model.fit(X_train, X_train_label, eval_set=eval_set, eval_metric=f1_weighted, verbose=10, early_stopping_rounds=150) \n",
    "    pred_train = model.predict_proba(X_val)\n",
    " \n",
    "    pred_test += model.predict_proba(test_x)  * 1.0 / spilts_num\n",
    "    \n",
    "    meta_train[te_ind] = pred_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6879744000000001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.688897,0.683757,0.689377,0.686895,0.68798,0.690323,0.69113,0.687716,0.689921,0.683748])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94358\n",
      "2     33725\n",
      "7     21548\n",
      "1     16468\n",
      "5     10451\n",
      "10     3597\n",
      "0      2142\n",
      "3      2114\n",
      "9      1874\n",
      "4      1347\n",
      "6       553\n",
      "11      336\n",
      "8       203\n",
      "Name: recommend_mode, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame()\n",
    "result['sid'] = data_[test_index]['sid']\n",
    "result['recommend_mode'] = np.argmax(pred_test,axis=1)\n",
    "result['recommend_mode'] = result['recommend_mode'].astype(int)\n",
    "print(len(result))\n",
    "print(result['recommend_mode'].value_counts())\n",
    "result[['sid', 'recommend_mode']].to_csv('10fold_baseline.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "上面可以调节权重的范围,就可以达到线上top10的成绩了,大家也可以按照大顺说的将分布不一致的数据删除,就可以在复赛稳定在top10了。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
